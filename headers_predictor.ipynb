{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import re\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_PATTERN = \"-/():.,;\"\n",
    "YEAR_PATTERN = r\"\\b(19|20)\\d{2}\\b\"  # 1900 - 2099\n",
    "DDMM_PATTERN = r\"\\d{2}/\\d{2}\"  # 00/00 - 99/99\n",
    "\n",
    "punct_remover = str.maketrans(PUNCT_PATTERN, \" \" * len(PUNCT_PATTERN))\n",
    "\n",
    "\n",
    "def words_from_values(values):\n",
    "    \"\"\"\n",
    "    Extract words from values\n",
    "    \"\"\"\n",
    "    \n",
    "    return [y for y in sum([x.lower().translate(punct_remover).split() for x in values], []) if y and not re.search(r\"\\d\", y)]\n",
    "\n",
    "\n",
    "def get_words(data):\n",
    "    \"\"\"\n",
    "    Collect all the words extracted from values for the whole provided data\n",
    "    \"\"\"\n",
    "\n",
    "    words = []\n",
    "    for obj in tqdm(data):\n",
    "        words.extend(words_from_values(obj['values']))\n",
    "        \n",
    "    return words\n",
    "\n",
    "\n",
    "def words_counter(data):\n",
    "    \"\"\"\n",
    "    Count popularity of words of provided data\n",
    "    \"\"\"\n",
    "\n",
    "    words = get_words(data)\n",
    "    \n",
    "    return {x: y / len(words) for x, y in Counter(words).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "with open('./document-standardization-training-dataset.txt', 'r', encoding=\"utf8\") as f:\n",
    "    data = [json.loads(x) for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56034027af2b44559ab8d73471842adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14932.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Concat all row of the data\n",
    "data_all = []\n",
    "for row in tqdm(data):\n",
    "    data_all.extend(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a subset of the data for easier demostration\n",
    "sample_sz = 150000\n",
    "data_sampled = np.random.choice(data_all, size=sample_sz, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify data structure\n",
    "\n",
    "```json\n",
    "{'values': [\n",
    "    {'value':'a'}, \n",
    "    {'value': 'b'}\n",
    "  ],\n",
    "  'type': 'HEADERS'\n",
    "}\n",
    "\n",
    "to\n",
    "\n",
    "{'values': ['a', 'b'], 'type': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sampled = [{'values':[x['value'] for x in x['values']], 'type': x['type'] == 'HEADERS'} for x in data_sampled]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis and feature extraction\n",
    "\n",
    "At first glance, several noticeable characteristics can be observed in the data, which distinguish HEADERS from other objects.\n",
    "1. Sparcity: amount of empty values\n",
    "2. Number of floats\n",
    "3. Number of long ints\n",
    "4. Size is not distinguishing by itself, but working other features it could make sence for boosting algorithm\n",
    "\n",
    "We are starting with collecting that features\n",
    "\n",
    "<i>* It is important to note that all the features can be collected using a single loop. However, for presentation purposes, and better explanation, we have separated them into multiple loops.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty dataframe to collect features\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting first features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c52f0ab70a42879be5746e7a2c936c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "types = []\n",
    "sparsity = []\n",
    "floats_N = []\n",
    "long_ints = []\n",
    "size = []\n",
    "for row in tqdm(data_sampled):\n",
    "    types.append(row['type'])\n",
    "    \n",
    "    values = np.array(row['values'])\n",
    "    \n",
    "    size.append(len(values))\n",
    "    \n",
    "    sparsity.append((values == '').sum() / len(values))\n",
    "    \n",
    "    floats_N.append(sum(np.vectorize(lambda x: x.replace('.', '', 1).isdigit() and '.' in x)(values)) / len(values))\n",
    "    \n",
    "    long_ints.append(sum(np.vectorize(lambda x: x.replace('.', '', 1).isdigit() and len(x) > 4)(values)) / len(values))\n",
    "    \n",
    "df['y'] = types\n",
    "df['sparsity'] = sparsity\n",
    "df['floats_N'] = floats_N\n",
    "df['long_ints'] = long_ints\n",
    "df['size'] = size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linguistic patterns detection\n",
    "Second thing to come in mind is to find additional linguistic patterns distinguishing HEADERS from others. First of all separate data to HEADERS and not HEADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objects with type='HEADERS'\n",
    "data_headers = [x for x in data_sampled if x['type']]\n",
    "# Other objects\n",
    "data_others = [x for x in data_sampled if not x['type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get words popularity in each sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b5e2714a1e4177ae9ba79160da8ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4378.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "header_words = words_counter(data_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89859aba83424fdda87d5a4042cde016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=145622.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "other_words = words_counter(data_others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate the weight of the word in sense of how it correspond to HEADERS by following formula:\n",
    "\n",
    "$ph$ - word popularity in headers\n",
    "\n",
    "$po$ - word popularity in others\n",
    "$$\n",
    "weight = \\frac{ph-po}{ph+po}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54beeab364784324b9c956feab44c753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_words = set(get_words(data_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weights = {}\n",
    "for word in all_words:\n",
    "    h = header_words.get(word, 0)\n",
    "    o = other_words.get(word, 0) \n",
    "    word_weights[word] = (h - o) / (h + o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we get mean, max and std vaues of weights of all words or the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62eeeb42550346368b42d2d78f9c4ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_weights_mean = []\n",
    "word_weights_max = []\n",
    "word_weights_std = []\n",
    "for row in tqdm(data_sampled):\n",
    "    values = np.array(row['values'])\n",
    "    \n",
    "    word_weights_ = np.array([word_weights.get(word, 0) for word in words_from_values(values)])\n",
    "    if len(word_weights_) > 0:\n",
    "        word_weights_mean.append(word_weights_.mean())\n",
    "        word_weights_max.append(word_weights_.max())\n",
    "        word_weights_std.append(word_weights_.std())\n",
    "    else:\n",
    "        word_weights_mean.append(np.nan)\n",
    "        word_weights_max.append(np.nan)\n",
    "        word_weights_std.append(np.nan)\n",
    "        \n",
    "df['word_weights_mean'] = word_weights_mean\n",
    "df['word_weights_max'] = word_weights_max\n",
    "df['word_weights_std'] = word_weights_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dates pattern detection\n",
    "Moving forward, we notice that headers object contains dates in format of 'yyyy' and 'dd/mm'. So we add two features more denoting amount of dates in the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66f9d6c63b34f17bdbcd134a64e63aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "years_N = []\n",
    "ddmm_N = []\n",
    "for row in tqdm(data_sampled):\n",
    "    values = np.array(row['values'])\n",
    "        \n",
    "    string = ' '.join(values)\n",
    "    if len(string) > 0:\n",
    "        years_N.append(len(re.findall(YEAR_PATTERN, string)) / len(string))\n",
    "        ddmm_N.append(len(re.findall(DDMM_PATTERN, string)) / len(string))\n",
    "    else:\n",
    "        years_N.append(np.nan)\n",
    "        ddmm_N.append(np.nan)\n",
    "        \n",
    "df['years_N'] = years_N\n",
    "df['ddmm_N'] = ddmm_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "It is known that gradient boosting algorithms performs better for such kind of data. We get LightGMB classifier as fastest performing for such kind of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('y', axis=1).drop('size', axis=1)\n",
    "y = df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(max_depth=10, n_estimators=500, num_leaves=20)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(max_depth=10, n_estimators=500, num_leaves=20)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(max_depth=10, n_estimators=500, num_leaves=20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LGBMClassifier(boosting_type='gbdt', num_leaves=20, max_depth=10, learning_rate=0.1, n_estimators=500)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "We get simple accuracy score for testing our result (true / all). As we have strongly inbalanced data it's reasonable to compare result with dummy classifier, which just predict no HEADERS for all objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9940333333333333\n",
      "Dummy Acc.: 0.9714\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc_fake = accuracy_score(y_test, [False]* len(y_test))\n",
    "print(\"Accuracy:\", acc)\n",
    "print('Dummy Acc.:', acc_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also it's reasonable to test accuracy only for HEADERS, i.e. how much HEADERS was correctly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8578088578088578\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score([1] * y_test.sum(), y_pred[y_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better understanding of our model performance, precission, recall and f1-score could be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.9281210592686002\n",
      "recall 0.8578088578088578\n",
      "f1-score 0.8915808600847971\n"
     ]
    }
   ],
   "source": [
    "print('precision', precision_score(y_test, y_pred))\n",
    "print('recall', recall_score(y_test, y_pred))\n",
    "print('f1-score', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that for specific needs and requirements, it may be necessary to use different evaluation metrics to ensure that the model meets the desired objectives. For example, if correctly classifying all HEADERS is critical, while the correct classification of other labels is less important, then the model's balance may need to be adjusted accordingly to optimize for this specific metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future improvements\n",
    "\n",
    "1. Like in the majority of machine learning models, increasing the size of the training data can lead to improved performance and reduced overfitting. This is because a larger dataset provides a more diverse range of examples for the model to learn from, helping it to better generalize to new, unseen data.\n",
    "2. Basic data object descriptions improve data understanding and meaningful pattern identification, leading to better performance and more accurate predictions.\n",
    "3. Using advanced functions and aggregation methods (other than max, mean, std) to calculate words' weights.\n",
    "4. Search linguistic patterns not only to words but also to word2vecs, so the model will better work in case of unseen words.\n",
    "5. Search linguistic patterns not to words but to values. Better cleanup teqniques should be implemented.\n",
    "6. Create word classes (e.g. Jan, Feb, ... to class month) and search linguistic patterns in classes.\n",
    "7. Continuosly browsing other models, playing with parameters and fine tuning the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
